{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec56339",
   "metadata": {
    "id": "bec56339"
   },
   "source": [
    "# Import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46631608",
   "metadata": {
    "id": "46631608"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from threading import Thread   # for thread creation\n",
    "\n",
    "import torch\n",
    "\n",
    "import fitz  # PyMuPDF   for pdf file reading \n",
    "\n",
    "import faiss  # to find chuck cosine similiarity using Eculediean distance \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel  # for chunk embeddings \n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer  #pretrained model used for generation or synthesis \n",
    "\n",
    "\n",
    "import logging\n",
    "from flask import Flask, request, jsonify, render_template  # to Create Web server \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"IPython.core.interactiveshell\")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Used Machine for the assignment = {device}')\n",
    "\n",
    "#This is  to handle issues related to multiple instances of the OpenMP runtime in Python applications.\n",
    "# This is often done to avoid runtime errors when using libraries that rely on OpenMP for parallel processing, such as NumPy or FAISS    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef4c0a-56a9-478b-9d67-c6ca0d5bc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration Parameters \n",
    "chunk_size = 16\n",
    "chunk_overlap = 4\n",
    "K = 3  ## Number of nearest chunks to retrieve\n",
    "index_file = 'vector_index.faiss'\n",
    "\n",
    "#UI web server settings \n",
    "host_name = '127.0.0.1'  #currently set to run in localhost machine\n",
    "port_no = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e0cb",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "# Data Acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51d895",
   "metadata": {
    "id": "4b51d895"
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "def get_doc_list(file_dir):\n",
    "    doc_list = [os.path.join(file_dir, file) for file\n",
    "                 in sorted(os.listdir(file_dir)) if file[-4:] == '.pdf'] \n",
    "    return doc_list\n",
    "   \n",
    "def read_pdf(file_path):\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in document:  # Iterate through each page\n",
    "        text += page.get_text()  # Extract text from the current page\n",
    "    document.close()\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158db56-26ac-4d98-9f49-77d6d237ba5e",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "#  Ingestion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8a823-5ab1-4603-8810-0b1c70fce91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion\n",
    "\n",
    "def chunk_text_with_overlap(text, chunksize, overlap):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create chunks with overlap\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunksize - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunksize])\n",
    "        chunks.append(chunk)\n",
    "        if i + chunksize >= len(words):\n",
    "            break  # Stop if the next chunk goes out of bounds\n",
    "    \n",
    "    return chunks\n",
    "    import torch\n",
    "\n",
    "\n",
    "# Load the tokenizer and model for  chunk embeddings \n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Example model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            outputs = model(**inputs)\n",
    "            # Use the mean of the last hidden states as the chunk embedding\n",
    "            chunk_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            embeddings.append(chunk_embedding)\n",
    "    return embeddings\n",
    "\n",
    "def display_chunks_embeddings(chunks,chunk_embeddings):\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):  \n",
    "        print(f\"Chunk {i + 1}: {chunk}\") \n",
    "\n",
    "#Vector DB to store chunks into vector Database, here FAISS  is used\n",
    "def do_indexing(chunk_embeddings):\n",
    "    embedding_dimension = chunk_embeddings[0].shape[0]\n",
    "    print(embedding_dimension)\n",
    "\n",
    "    # Create a FAISS index\n",
    "    index = faiss.IndexFlatL2(embedding_dimension)\n",
    "\n",
    "    # Convert chunk embeddings to a 2D numpy array\n",
    "    chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "    # Ensure the shape is correct\n",
    "    print(\"Shape of chunk_embeddings:\", chunk_embeddings.shape)  # Should be (n, d)\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(chunk_embeddings)\n",
    "    # Store the index to disk\n",
    "    faiss.write_index(index, index_file)\n",
    "    return index\n",
    "\n",
    "def get_stored_index(index_file):\n",
    "    # Load the index from disk\n",
    "    loaded_index = faiss.read_index(index_file)\n",
    "    return loaded_index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f97386-b0d9-4a7b-9da6-20b78e7aab14",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "#  Retreival "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930952df-3797-4ce8-b50c-f812e7711a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval \n",
    "def retrieve_chunks(index,query, k):\n",
    "    query_embedding = get_embeddings([query])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    print(\"Shape of query_embedding:\", query_embedding.shape)\n",
    "    D, I = index.search(query_embedding, k)  # D: distances, I: indices\n",
    "    retreived_chunk_list=[]\n",
    "    for i in range(k):\n",
    "        retreived_chunk_list.append(chunks[I[0][i]])\n",
    "    #print(retreived_chunk_list)\n",
    "    return retreived_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4f212-5941-4e9d-a75e-96e4ed04d85a",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "#  Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4857a1-9b0c-49a1-b2c0-69c4ca8ef03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthesis or Generation\n",
    "# Load the tokenizer and model\n",
    "gpt_model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)\n",
    "gpt_model = GPTNeoForCausalLM.from_pretrained(gpt_model_name)\n",
    "\n",
    "def generate_response(index ,query,k):\n",
    "    retrieved_chuncks = retrieve_chunks(index,query, k)\n",
    "    context = \" \".join(retrieved_chuncks)\n",
    "    input_text = f\"Context: {context}\\nQuery: {query}\\nResponse:\"\n",
    "    #print(input_text)  \n",
    "    inputs = gpt_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    #print(inputs)\n",
    "    outputs = gpt_model.generate(**inputs, max_length=100)    \n",
    "    return gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46734f22-666b-4464-9afb-34768e47c2c0",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "#  Web Server  (User Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7fcb3-2145-4c90-adc7-80bd31a2cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__, template_folder=os.path.abspath('templates'))\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def process_question():\n",
    "    print('process_question()') \n",
    "    try:\n",
    "        question = request.json['question']\n",
    "        app.logger.info(f\"Received question: {question}\")\n",
    "        print(question)\n",
    "        answer = generate_response(index,question,K)\n",
    "        app.logger.info(f\"Generated answer: {answer}\")\n",
    "        return jsonify({'answer': answer})\n",
    "    except Exception as e:\n",
    "        app.logger.error(f\"An error occurred: {str(e)}\", exc_info=True)\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "def run_app():\n",
    "    app.run(debug=True, use_reloader=False, host=host_name, port=f'{port_no}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a314f19-8ff2-41a2-a389-29390cab8928",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "#  Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebf521-d1d2-4ff8-814e-95a039bb6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    doc_dir = './Data'\n",
    "    doc_list = get_doc_list(doc_dir)\n",
    "    print(doc_list)\n",
    "    doc_content = read_pdf(doc_list[0])\n",
    "    #print(doc_content)\n",
    "    \n",
    "    #Create chunks \n",
    "    chunks = chunk_text_with_overlap(doc_content, chunk_size, chunk_overlap)\n",
    "\n",
    "    # Create embeddings for the chunks\n",
    "    chunk_embeddings = get_embeddings(chunks)\n",
    "\n",
    "    #Indexing\n",
    "    index = do_indexing(chunk_embeddings)\n",
    "\n",
    "    thread = Thread(target=run_app)\n",
    "    thread.start()\n",
    "\n",
    "    print(f\"Flask server is running on http://{host_name}:{port_no}\")\n",
    "\n",
    "    '''\n",
    "    # Example usage\n",
    "    query = \"what is natural language processing\"\n",
    "    response = generate_response(index,query,K)\n",
    "    print(response)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
